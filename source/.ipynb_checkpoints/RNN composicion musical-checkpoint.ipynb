{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Author: Nicolas Boulanger-Lewandowski\n",
    "# University of Montreal (2012)\n",
    "# RNN-RBM deep learning tutorial\n",
    "# More information at http://deeplearning.net/tutorial/rnnrbm.html\n",
    "# Updated 2017: Jorge Drot de Gourville\n",
    "\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy\n",
    "try:\n",
    "    import pylab\n",
    "except ImportError:\n",
    "    print (\"pylab isn't available. If you use its functionality, it will crash.\")\n",
    "    print(\"It can be installed with 'pip install -q Pillow'\")\n",
    "\n",
    "from midi.utils import midiread, midiwrite\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Don't use a python long as this don't work on 32 bits computers.\n",
    "numpy.random.seed(0xbeef)\n",
    "rng = RandomStreams(seed=numpy.random.randint(1 << 30))\n",
    "theano.config.warn.subtensor_merge_bug = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_rbm(v, W, bv, bh, k):\n",
    "    '''Construct a k-step Gibbs chain starting at v for an RBM.\n",
    "\n",
    "    v : Theano vector or matrix\n",
    "        If a matrix, multiple chains will be run in parallel (batch).\n",
    "    W : Theano matrix\n",
    "        Weight matrix of the RBM.\n",
    "    bv : Theano vector\n",
    "        Visible bias vector of the RBM.\n",
    "    bh : Theano vector\n",
    "        Hidden bias vector of the RBM.\n",
    "    k : scalar or Theano scalar\n",
    "        Length of the Gibbs chain.\n",
    "\n",
    "    Return a (v_sample, cost, monitor, updates) tuple:\n",
    "\n",
    "    v_sample : Theano vector or matrix with the same shape as `v`\n",
    "        Corresponds to the generated sample(s).\n",
    "    cost : Theano scalar\n",
    "        Expression whose gradient with respect to W, bv, bh is the CD-k\n",
    "        approximation to the log-likelihood of `v` (training example) under the\n",
    "        RBM. The cost is averaged in the batch case.\n",
    "    monitor: Theano scalar\n",
    "        Pseudo log-likelihood (also averaged in the batch case).\n",
    "    updates: dictionary of Theano variable -> Theano variable\n",
    "        The `updates` object returned by scan.'''\n",
    "    \n",
    "    def gibbs_step(v):\n",
    "        mean_h = T.nnet.sigmoid(T.dot(v, W) + bh)\n",
    "        h = rng.binomial(size=mean_h.shape, n=1, p=mean_h,\n",
    "                         dtype=theano.config.floatX)\n",
    "        mean_v = T.nnet.sigmoid(T.dot(h, W.T) + bv)\n",
    "        v = rng.binomial(size=mean_v.shape, n=1, p=mean_v,\n",
    "                         dtype=theano.config.floatX)\n",
    "        return mean_v, v\n",
    "\n",
    "    chain, updates = theano.scan(lambda v: gibbs_step(v)[1], outputs_info=[v],\n",
    "                                 n_steps=k)\n",
    "    v_sample = chain[-1]\n",
    "\n",
    "    mean_v = gibbs_step(v_sample)[0]\n",
    "    monitor = T.xlogx.xlogy0(v, mean_v) + T.xlogx.xlogy0(1 - v, 1 - mean_v)\n",
    "    monitor = monitor.sum() / v.shape[0]\n",
    "\n",
    "    def free_energy(v):\n",
    "        return -(v * bv).sum() - T.log(1 + T.exp(T.dot(v, W) + bh)).sum()\n",
    "    cost = (free_energy(v) - free_energy(v_sample)) / v.shape[0]\n",
    "\n",
    "    return v_sample, cost, monitor, updates\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " def shared_normal(num_rows, num_cols, scale=1):\n",
    "    '''Initialize a matrix shared variable with normally distributed\n",
    "    elements.'''\n",
    "    return theano.shared(numpy.random.normal(\n",
    "        scale=scale, size=(num_rows, num_cols)).astype(theano.config.floatX))\n",
    "\n",
    "\n",
    "def shared_zeros(*shape):\n",
    "    '''Initialize a vector shared variable with zero elements.'''\n",
    "    return theano.shared(numpy.zeros(shape, dtype=theano.config.floatX))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_rnnrbm(n_visible, n_hidden, n_hidden_recurrent):\n",
    "    '''Construct a symbolic RNN-RBM and initialize parameters.\n",
    "\n",
    "    n_visible : integer\n",
    "        Number of visible units.\n",
    "    n_hidden : integer\n",
    "        Number of hidden units of the conditional RBMs.\n",
    "    n_hidden_recurrent : integer\n",
    "        Number of hidden units of the RNN.\n",
    "\n",
    "    Return a (v, v_sample, cost, monitor, params, updates_train, v_t,\n",
    "    updates_generate) tuple:\n",
    "\n",
    "    v : Theano matrix\n",
    "        Symbolic variable holding an input sequence (used during training)\n",
    "    v_sample : Theano matrix\n",
    "        Symbolic variable holding the negative particles for CD log-likelihood\n",
    "        gradient estimation (used during training)\n",
    "    cost : Theano scalar\n",
    "        Expression whose gradient (considering v_sample constant) corresponds\n",
    "        to the LL gradient of the RNN-RBM (used during training)\n",
    "    monitor : Theano scalar\n",
    "        Frame-level pseudo-likelihood (useful for monitoring during training)\n",
    "    params : tuple of Theano shared variables\n",
    "        The parameters of the model to be optimized during training.\n",
    "    updates_train : dictionary of Theano variable -> Theano variable\n",
    "        Update object that should be passed to theano.function when compiling\n",
    "        the training function.\n",
    "    v_t : Theano matrix\n",
    "        Symbolic variable holding a generated sequence (used during sampling)\n",
    "    updates_generate : dictionary of Theano variable -> Theano variable\n",
    "        Update object that should be passed to theano.function when compiling\n",
    "        the generation function.'''\n",
    "\n",
    "    W = shared_normal(n_visible, n_hidden, 0.01)\n",
    "    bv = shared_zeros(n_visible)\n",
    "    bh = shared_zeros(n_hidden)\n",
    "    Wuh = shared_normal(n_hidden_recurrent, n_hidden, 0.0001)\n",
    "    Wuv = shared_normal(n_hidden_recurrent, n_visible, 0.0001)\n",
    "    Wvu = shared_normal(n_visible, n_hidden_recurrent, 0.0001)\n",
    "    Wuu = shared_normal(n_hidden_recurrent, n_hidden_recurrent, 0.0001)\n",
    "    bu = shared_zeros(n_hidden_recurrent)\n",
    "\n",
    "    params = W, bv, bh, Wuh, Wuv, Wvu, Wuu, bu  # learned parameters as shared\n",
    "                                                # variables\n",
    "\n",
    "    v = T.matrix()  # a training sequence\n",
    "    u0 = T.zeros((n_hidden_recurrent,))  # initial value for the RNN hidden\n",
    "                                         # units\n",
    "\n",
    "    # If `v_t` is given, deterministic recurrence to compute the variable\n",
    "    # biases bv_t, bh_t at each time step. If `v_t` is None, same recurrence\n",
    "    # but with a separate Gibbs chain at each time step to sample (generate)\n",
    "    # from the RNN-RBM. The resulting sample v_t is returned in order to be\n",
    "    # passed down to the sequence history.\n",
    "    def recurrence(v_t, u_tm1):\n",
    "        bv_t = bv + T.dot(u_tm1, Wuv)\n",
    "        bh_t = bh + T.dot(u_tm1, Wuh)\n",
    "        generate = v_t is None\n",
    "        if generate:\n",
    "            v_t, _, _, updates = build_rbm(T.zeros((n_visible,)), W, bv_t,\n",
    "                                           bh_t, k=25)\n",
    "        u_t = T.tanh(bu + T.dot(v_t, Wvu) + T.dot(u_tm1, Wuu))\n",
    "        return ([v_t, u_t], updates) if generate else [u_t, bv_t, bh_t]\n",
    "\n",
    "    # For training, the deterministic recurrence is used to compute all the\n",
    "    # {bv_t, bh_t, 1 <= t <= T} given v. Conditional RBMs can then be trained\n",
    "    # in batches using those parameters.\n",
    "    (u_t, bv_t, bh_t), updates_train = theano.scan(\n",
    "        lambda v_t, u_tm1, *_: recurrence(v_t, u_tm1),\n",
    "        sequences=v, outputs_info=[u0, None, None], non_sequences=params)\n",
    "    v_sample, cost, monitor, updates_rbm = build_rbm(v, W, bv_t[:], bh_t[:],\n",
    "                                                     k=15)\n",
    "    updates_train.update(updates_rbm)\n",
    "\n",
    "    # symbolic loop for sequence generation\n",
    "    (v_t, u_t), updates_generate = theano.scan(\n",
    "        lambda u_tm1, *_: recurrence(None, u_tm1),\n",
    "        outputs_info=[None, u0], non_sequences=params, n_steps=200)\n",
    "\n",
    "    return (v, v_sample, cost, monitor, params, updates_train, v_t,\n",
    "\n",
    "            updates_generate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class RnnRbm:\n",
    "    '''Simple class to train an RNN-RBM from MIDI files and to generate sample\n",
    "    sequences.'''\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_hidden=150,\n",
    "        n_hidden_recurrent=100,\n",
    "        lr=0.001,\n",
    "        r=(21, 109), #r=(36, 95),     \n",
    "        dt=0.15\n",
    "    ):\n",
    "        '''Constructs and compiles Theano functions for training and sequence\n",
    "        generation.\n",
    "\n",
    "        n_hidden : integer\n",
    "            Number of hidden units of the conditional RBMs.\n",
    "        n_hidden_recurrent : integer\n",
    "            Number of hidden units of the RNN.\n",
    "        lr : float\n",
    "            Learning rate\n",
    "        r : (integer, integer) tuple\n",
    "            Specifies the pitch range of the piano-roll in MIDI note numbers,\n",
    "            including r[0] but not r[1], such that r[1]-r[0] is the number of\n",
    "            visible units of the RBM at a given time step. The default (21,\n",
    "            109) corresponds to the full range of piano (88 notes).\n",
    "            reducido a 36-95  Do de C2 a Si de C6 para evitar notas outliers\n",
    "            para solo melodia de clave de sol de 60-95  (C4-C6)\n",
    "            para acompañamiento clave fa (36-60)  (C2-C3)\n",
    "             ojo que si un midi de entrenamiento tiene notas fuera de rango da error\n",
    "        dt : float\n",
    "            Sampling period when converting the MIDI files into piano-rolls, or\n",
    "            equivalently the time difference between consecutive time steps.'''\n",
    "\n",
    "        self.r = r\n",
    "        self.dt = dt\n",
    "        (v, v_sample, cost, monitor, params, updates_train, v_t,\n",
    "            updates_generate) = build_rnnrbm(\n",
    "                r[1] - r[0],\n",
    "                n_hidden,\n",
    "                n_hidden_recurrent\n",
    "            )\n",
    "\n",
    "        gradient = T.grad(cost, params, consider_constant=[v_sample])\n",
    "        updates_train.update(\n",
    "            ((p, p - lr * g) for p, g in zip(params, gradient))\n",
    "        )\n",
    "        self.train_function = theano.function(\n",
    "            [v],\n",
    "            monitor,\n",
    "            updates=updates_train\n",
    "        )\n",
    "        self.generate_function = theano.function(\n",
    "            [],\n",
    "            v_t,\n",
    "            updates=updates_generate\n",
    "        )\n",
    "\n",
    "    def train(self, files, batch_size=100, num_epochs=200):\n",
    "        '''Train the RNN-RBM via stochastic gradient descent (SGD) using MIDI\n",
    "        files converted to piano-rolls.\n",
    "\n",
    "        files : list of strings\n",
    "            List of MIDI files that will be loaded as piano-rolls for training.\n",
    "        batch_size : integer\n",
    "            Training sequences will be split into subsequences of at most this\n",
    "            size before applying the SGD updates.\n",
    "        num_epochs : integer\n",
    "            Number of epochs (pass over the training set) performed. The user\n",
    "            can safely interrupt training with Ctrl+C at any time.'''\n",
    "\n",
    "        assert len(files) > 0, 'Training set is empty!' \\\n",
    "                               ' (did you download the data files?)'\n",
    "            \n",
    "        allFilesProcessed = False\n",
    "        while not allFilesProcessed:\n",
    "            try:    \n",
    "                #carga los midis en formato piano roll\n",
    "                dataset = [midiread(f, self.r,\n",
    "                                self.dt).piano_roll.astype(theano.config.floatX)\n",
    "                       for f in files]\n",
    "                allFilesProcessed = True\n",
    "\n",
    "            except:\n",
    "                    #si el archivo genera un error lo renombra a .err en disco, \n",
    "                    #lo elimina de la lista de midis y reintenta\n",
    "                    \n",
    "                    print (\"File: \" + f)\n",
    "                    os.rename(f, f + \".err\")\n",
    "                    files.remove(f)\n",
    "                    continue\n",
    "\n",
    "        try:\n",
    "            for epoch in range(num_epochs):\n",
    "                numpy.random.shuffle(dataset)\n",
    "                costs = []\n",
    "\n",
    "                for s, sequence in enumerate(dataset):\n",
    "                    for i in range(0, len(sequence), batch_size):\n",
    "                        cost = self.train_function(sequence[i:i + batch_size])\n",
    "                        costs.append(cost)\n",
    "\n",
    "                print('Epoch %i/%i' % (epoch + 1, num_epochs))\n",
    "                print(numpy.mean(costs))\n",
    "                sys.stdout.flush()\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            print('Interrupted by user.')\n",
    "\n",
    "    def generate(self, filename, show=False):\n",
    "        '''Generate a sample sequence, plot the resulting piano-roll and save\n",
    "        it as a MIDI file.\n",
    "\n",
    "        filename : string\n",
    "            A MIDI file will be created at this location.\n",
    "        show : boolean\n",
    "            If True, a piano-roll of the generated sequence will be shown.'''\n",
    "        #print (filename)\n",
    "\n",
    "        piano_roll = self.generate_function()\n",
    "        midiwrite(filename, piano_roll, self.r, self.dt)\n",
    "        if show:\n",
    "            extent = (0, self.dt * len(piano_roll)) + self.r\n",
    "            pylab.figure()\n",
    "            pylab.imshow(piano_roll.T, origin='lower', aspect='auto',\n",
    "                         interpolation='nearest', cmap=pylab.cm.gray_r,\n",
    "                         extent=extent)\n",
    "            pylab.xlabel('time (s)')\n",
    "            pylab.ylabel('MIDI note number')\n",
    "            pylab.title('generated piano-roll')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_rnnrbm(directory, dataset , batch_size=100, num_epochs=200):\n",
    "    model = RnnRbm()\n",
    "    print (directory)\n",
    "    re = os.path.join(os.path.split(os.path.dirname(directory))[0],\n",
    "                      'data', dataset, 'train', '*.mid')\n",
    "    print (re)\n",
    "    model.train(glob.glob(re),\n",
    "                batch_size=batch_size, num_epochs=num_epochs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jorge\\Documents\\Proyectos\\IAAR\\GitHub\\ResucitandoAlMaestro\\config\\config.ini\n",
      "TangoClasico\n",
      "10\n",
      "50\n",
      "comienza entrenamiento 20170305-230401\n",
      "C:\\Users\\Jorge\\Documents\\Proyectos\\IAAR\\GitHub\\ResucitandoAlMaestro\\source\\\n",
      "C:\\Users\\Jorge\\Documents\\Proyectos\\IAAR\\GitHub\\ResucitandoAlMaestro\\data\\TangoClasico\\train\\*.mid\n",
      "Epoch 1/50\n",
      "-11.1380957158\n",
      "Epoch 2/50\n",
      "-10.4784806086\n",
      "Epoch 3/50\n",
      "-10.4785877763\n",
      "Epoch 4/50\n",
      "-10.4882803426\n",
      "Epoch 5/50\n",
      "-10.4600520623\n",
      "Epoch 6/50\n",
      "-10.2772032754\n",
      "Epoch 7/50\n",
      "-10.025732997\n",
      "Epoch 8/50\n",
      "-9.81599533719\n",
      "Epoch 9/50\n",
      "-9.5073479643\n",
      "Epoch 10/50\n",
      "-9.21676413046\n",
      "Epoch 11/50\n",
      "-8.9811022443\n",
      "Epoch 12/50\n",
      "-8.76873933855\n",
      "Epoch 13/50\n",
      "-8.55333901733\n",
      "Epoch 14/50\n",
      "-8.328123264\n",
      "Epoch 15/50\n",
      "-8.1319224971\n",
      "Epoch 16/50\n",
      "-7.96987661484\n",
      "Epoch 17/50\n",
      "-7.81893995619\n",
      "Epoch 18/50\n",
      "-7.69957624761\n",
      "Epoch 19/50\n",
      "-7.59476485524\n",
      "Epoch 20/50\n",
      "-7.48131558409\n",
      "Epoch 21/50\n",
      "-7.39642929013\n",
      "Epoch 22/50\n",
      "-7.30863573539\n",
      "Epoch 23/50\n",
      "-7.2389565917\n",
      "Epoch 24/50\n",
      "-7.17989338773\n",
      "Epoch 25/50\n",
      "-7.11756414178\n",
      "Epoch 26/50\n",
      "-7.07640472245\n",
      "Epoch 27/50\n",
      "-7.02885558114\n",
      "Epoch 28/50\n",
      "-6.99509288406\n",
      "Epoch 29/50\n",
      "-6.95299775476\n",
      "Epoch 30/50\n",
      "-6.9302584287\n",
      "Epoch 31/50\n",
      "-6.89858200766\n",
      "Epoch 32/50\n",
      "-6.87727046608\n",
      "Epoch 33/50\n",
      "-6.86063025414\n",
      "Epoch 34/50\n",
      "-6.84191509933\n",
      "Epoch 35/50\n",
      "-6.83435406079\n",
      "Epoch 36/50\n",
      "-6.81774228997\n",
      "Epoch 37/50\n",
      "-6.81003965807\n",
      "Epoch 38/50\n",
      "-6.80423287652\n",
      "Epoch 39/50\n",
      "-6.80412009703\n",
      "Epoch 40/50\n",
      "-6.79970267662\n",
      "Epoch 41/50\n",
      "-6.79980843418\n",
      "Epoch 42/50\n",
      "-6.80142797851\n",
      "Epoch 43/50\n",
      "-6.80623470832\n",
      "Epoch 44/50\n",
      "-6.81890967127\n",
      "Epoch 45/50\n",
      "-6.82498876216\n",
      "Epoch 46/50\n",
      "-6.8261903714\n",
      "Epoch 47/50\n",
      "-6.83849348587\n",
      "Epoch 48/50\n",
      "-6.84487634131\n",
      "Epoch 49/50\n",
      "-6.85759107712\n",
      "Epoch 50/50\n",
      "-6.86604487627\n",
      "fin del proceso 20170306-121515\n"
     ]
    }
   ],
   "source": [
    "#main\n",
    "\n",
    "import os\n",
    "import time\n",
    "import ConfigParser\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    directory = os.getcwd()+\"\\\\\"\n",
    "    configDirectory = os.path.join(os.path.split(os.path.dirname(directory))[0], 'config')\n",
    "    configFile = configDirectory + \"\\\\config.ini\" \n",
    "    print (configFile)\n",
    "    #read parameters from configuration file\n",
    "    Config = ConfigParser.ConfigParser()\n",
    "    Config.read(configFile)\n",
    "    dataset = Config.get('Parameters', 'dataset')\n",
    "    batch_size = int(Config.get('Parameters', 'batch_size'))\n",
    "    num_epochs = int(Config.get('Parameters', 'num_epochs'))\n",
    "    print (dataset)\n",
    "    print (batch_size)\n",
    "    print (num_epochs)\n",
    "   \n",
    "    outputDirectory = os.path.join(os.path.split(os.path.dirname(directory))[0], 'output')\n",
    "    print ('comienza entrenamiento ' + time.strftime(\"%Y%m%d-%H%M%S\"))\n",
    "    model = train_rnnrbm(directory, dataset, batch_size, num_epochs)\n",
    "    outputFile = outputDirectory + \"\\\\\" + 'gen_' + dataset + \"_\" + time.strftime(\"%Y%m%d-%H%M%S\") + '.mid'\n",
    "    model.generate(outputFile)\n",
    "    #pylab.show()\n",
    "    print ('fin del proceso ' + time.strftime(\"%Y%m%d-%H%M%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
